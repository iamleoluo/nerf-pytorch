# NeRF-pytorch


[NeRF](http://www.matthewtancik.com/nerf) (Neural Radiance Fields) is a method that achieves state-of-the-art results for synthesizing novel views of complex scenes. Here are some videos generated by this repository (pre-trained models are provided below):

![](https://user-images.githubusercontent.com/7057863/78472232-cf374a00-7769-11ea-8871-0bc710951839.gif)
![](https://user-images.githubusercontent.com/7057863/78472235-d1010d80-7769-11ea-9be9-51365180e063.gif)

This project is a faithful PyTorch implementation of [NeRF](http://www.matthewtancik.com/nerf) that **reproduces** the results while running **1.3 times faster**. The code is based on authors' Tensorflow implementation [here](https://github.com/bmild/nerf), and has been tested to match it numerically. 

## Installation

```
git clone https://github.com/yenchenlin/nerf-pytorch.git
cd nerf-pytorch
pip install -r requirements.txt
```

<details>
  <summary> Dependencies (click to expand) </summary>
  
  ## Dependencies
  - PyTorch 1.4
  - matplotlib
  - numpy
  - imageio
  - imageio-ffmpeg
  - configargparse
  
The LLFF data loader requires ImageMagick.

You will also need the [LLFF code](http://github.com/fyusion/llff) (and COLMAP) set up to compute poses if you want to run on your own real data.
  
</details>

## How To Run?

### Quick Start

Download data for two example datasets: `lego` and `fern`
```
bash download_example_data.sh
```

To train a low-res `lego` NeRF:
```
python run_nerf.py --config configs/lego.txt
```
After training for 100k iterations (~4 hours on a single 2080 Ti), you can find the following video at `logs/lego_test/lego_test_spiral_100000_rgb.mp4`.

![](https://user-images.githubusercontent.com/7057863/78473103-9353b300-7770-11ea-98ed-6ba2d877b62c.gif)

---

To train a low-res `fern` NeRF:
```
python run_nerf.py --config configs/fern.txt
```
After training for 200k iterations (~8 hours on a single 2080 Ti), you can find the following video at `logs/fern_test/fern_test_spiral_200000_rgb.mp4` and `logs/fern_test/fern_test_spiral_200000_disp.mp4`

![](https://user-images.githubusercontent.com/7057863/78473081-58ea1600-7770-11ea-92ce-2bbf6a3f9add.gif)

---

### More Datasets
To play with other scenes presented in the paper, download the data [here](https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1). Place the downloaded dataset according to the following directory structure:
```
├── configs                                                                                                       
│   ├── ...                                                                                     
│                                                                                               
├── data                                                                                                                                                                                                       
│   ├── nerf_llff_data                                                                                                  
│   │   └── fern                                                                                                                             
│   │   └── flower  # downloaded llff dataset                                                                                  
│   │   └── horns   # downloaded llff dataset
|   |   └── ...
|   ├── nerf_synthetic
|   |   └── lego
|   |   └── ship    # downloaded synthetic dataset
|   |   └── ...
```

---

To train NeRF on different datasets: 

```
python run_nerf.py --config configs/{DATASET}.txt
```

replace `{DATASET}` with `trex` | `horns` | `flower` | `fortress` | `lego` | etc.

---

To test NeRF trained on different datasets: 

```
python run_nerf.py --config configs/{DATASET}.txt --render_only
```

replace `{DATASET}` with `trex` | `horns` | `flower` | `fortress` | `lego` | etc.


### Pre-trained Models

You can download the pre-trained models [here](https://drive.google.com/drive/folders/1jIr8dkvefrQmv737fFm2isiT6tqpbTbv). Place the downloaded directory in `./logs` in order to test it later. See the following directory structure for an example:

```
├── logs 
│   ├── fern_test
│   ├── flower_test  # downloaded logs
│   ├── trex_test    # downloaded logs
```

### Reproducibility 

Tests that ensure the results of all functions and training loop match the official implentation are contained in a different branch `reproduce`. One can check it out and run the tests:
```
git checkout reproduce
py.test
```

## Method

[NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](http://tancik.com/nerf)  
 [Ben Mildenhall](https://people.eecs.berkeley.edu/~bmild/)\*<sup>1</sup>,
 [Pratul P. Srinivasan](https://people.eecs.berkeley.edu/~pratul/)\*<sup>1</sup>,
 [Matthew Tancik](http://tancik.com/)\*<sup>1</sup>,
 [Jonathan T. Barron](http://jonbarron.info/)<sup>2</sup>,
 [Ravi Ramamoorthi](http://cseweb.ucsd.edu/~ravir/)<sup>3</sup>,
 [Ren Ng](https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html)<sup>1</sup> <br>
 <sup>1</sup>UC Berkeley, <sup>2</sup>Google Research, <sup>3</sup>UC San Diego  
  \*denotes equal contribution  
  
<img src='imgs/pipeline.jpg'/>

> A neural radiance field is a simple fully connected network (weights are ~5MB) trained to reproduce input views of a single scene using a rendering loss. The network directly maps from spatial location and viewing direction (5D input) to color and opacity (4D output), acting as the "volume" so we can use volume rendering to differentiably render new views


## Citation
Kudos to the authors for their amazing results:
```
@misc{mildenhall2020nerf,
    title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
    author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
    year={2020},
    eprint={2003.08934},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
```

However, if you find this implementation or pre-trained models helpful, please consider to cite:
```
@misc{lin2020nerfpytorch,
  title={NeRF-pytorch},
  author={Yen-Chen, Lin},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished={\url{https://github.com/yenchenlin/nerf-pytorch/}},
  year={2020}
}
```

# NeRF C++ Implementation

This is a C++ implementation of [NeRF](http://www.matthewtancik.com/nerf) (Neural Radiance Fields), a method that achieves state-of-the-art results for synthesizing novel views of complex scenes.

## Dependencies

You can install dependencies either through Conda or system package manager.

### Using Conda (Recommended)

1. Install Miniconda or Anaconda if you haven't already.

2. Create and activate a new conda environment:
```bash
# Create new environment
conda create -n nerf-cpp python=3.8
conda activate nerf-cpp

# Install dependencies
conda install -c pytorch pytorch torchvision cudatoolkit=11.8
conda install -c conda-forge opencv eigen nlohmann_json
conda install -c conda-forge cmake ninja
```

3. Set environment variables for CMake:
```bash
export CMAKE_PREFIX_PATH=$CONDA_PREFIX
export Torch_DIR=$CONDA_PREFIX/lib/python3.8/site-packages/torch/share/cmake/Torch
```

### Using System Package Manager (Alternative)

#### Ubuntu/Debian

```bash
# Install system dependencies
sudo apt-get update
sudo apt-get install -y \
    build-essential \
    cmake \
    git \
    libopencv-dev \
    libeigen3-dev \
    nlohmann-json3-dev

# Install LibTorch (PyTorch C++ API)
wget https://download.pytorch.org/libtorch/cu118/libtorch-cxx11-abi-shared-with-deps-2.1.0%2Bcu118.zip
unzip libtorch-cxx11-abi-shared-with-deps-2.1.0+cu118.zip
sudo mv libtorch /usr/local/
```

#### macOS

```bash
# Install system dependencies using Homebrew
brew install cmake opencv eigen nlohmann-json

# Install LibTorch
wget https://download.pytorch.org/libtorch/cpu/libtorch-macos-2.1.0.zip
unzip libtorch-macos-2.1.0.zip
sudo mv libtorch /usr/local/
```

## Building the Project

1. Clone the repository:
```bash
git clone https://github.com/yourusername/nerf-cpp.git
cd nerf-cpp
```

2. Create a build directory and build the project:
```bash
mkdir build && cd build

# If using Conda
cmake -DCMAKE_PREFIX_PATH=$CONDA_PREFIX -DTorch_DIR=$CONDA_PREFIX/lib/python3.8/site-packages/torch/share/cmake/Torch ..

# If using system packages
cmake ..

make -j$(nproc)
```

## Running the Project

### Download Example Data

First, download the example datasets:

```bash
bash download_example_data.sh
```

This will download the `lego` and `fern` datasets to the `data` directory.

### Training

To train a NeRF model on the lego dataset:

```bash
./nerf_train configs/lego.txt
```

The training process will:
1. Load the dataset from `data/nerf_synthetic/lego`
2. Train the model for 100,000 iterations
3. Save checkpoints every 1,000 iterations
4. Save the final model as `final_model.pt`

### Configuration

The configuration files in the `configs` directory control various aspects of training:

- `expname`: Name of the experiment
- `basedir`: Directory to save logs and checkpoints
- `datadir`: Directory containing the dataset
- `dataset_type`: Type of dataset ("blender" or "llff")
- `N_samples`: Number of samples per ray
- `N_importance`: Number of importance samples
- `use_viewdirs`: Whether to use view-dependent effects
- `white_bkgd`: Whether to use white background

## Project Structure

```
nerf-cpp/
├── CMakeLists.txt
├── include/
│   └── nerf/
│       ├── model.hpp
│       ├── renderer.hpp
│       └── dataset.hpp
├── src/
│   ├── model.cpp
│   ├── renderer.cpp
│   └── dataset.cpp
├── examples/
│   └── train.cpp
├── configs/
│   ├── lego.txt
│   └── fern.txt
└── data/
    ├── nerf_synthetic/
    └── nerf_llff_data/
```

## Troubleshooting

### Common Issues

1. **CUDA not found**
   - Make sure you have CUDA installed
   - If using Conda, make sure you installed the correct CUDA toolkit version
   - Set `TORCH_CUDA_VERSION` in CMake if needed

2. **OpenCV not found**
   - If using Conda, make sure you activated the environment
   - If using system packages, install OpenCV development packages
   - Set `OpenCV_DIR` in CMake if needed

3. **Eigen3 not found**
   - If using Conda, make sure you activated the environment
   - If using system packages, install Eigen3 development packages
   - Set `EIGEN3_INCLUDE_DIR` in CMake if needed

### Memory Usage

The default configuration uses:
- Batch size: 1024 rays
- Samples per ray: 64
- Network width: 256
- Network depth: 8

You can adjust these parameters in the configuration files to reduce memory usage if needed.

## Citation

If you find this implementation helpful, please consider citing:

```bibtex
@misc{mildenhall2020nerf,
    title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
    author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
    year={2020},
    eprint={2003.08934},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
```
